{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Import",
   "id": "9dabd7dcfdeb0bc1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T11:58:20.353021Z",
     "start_time": "2025-11-07T11:58:19.185642Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "from typing import List, Optional\n",
    "\n",
    "import chromadb\n",
    "import httpx\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from pydantic import BaseModel\n",
    "from rich.console import Console\n",
    "from rich.table import Table\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential"
   ],
   "id": "8183ecff0f7f2c5f",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Variables et Configuration\n",
    "\n",
    "Centralisation de toutes les configurations pour faciliter l'évolution du projet. Modifiez ces variables pour adapter le comportement sans toucher au code."
   ],
   "id": "6691765e5bb0c84d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T11:58:20.358755Z",
     "start_time": "2025-11-07T11:58:20.356460Z"
    }
   },
   "cell_type": "code",
   "source": [
    "load_dotenv()\n",
    "\n",
    "# Clé API\n",
    "MISTRAL_API_KEY = os.getenv(\"MISTRAL_API_KEY\")\n",
    "\n",
    "# Configuration générale\n",
    "CONFIG = {\n",
    "    \"model\": \"mistral-small-2506\",\n",
    "    \"base_url\": \"https://api.mistral.ai/v1\",\n",
    "    \"chat_endpoint\": \"/chat/completions\",\n",
    "    \"embed_model\": \"mistral-embed\",\n",
    "    \"chroma_path\": \"./chroma_db\",\n",
    "    \"rag_n_results\": 3,\n",
    "    \"default_temperature\": 0.2,\n",
    "    \"default_max_tokens\": 512,\n",
    "    \"retry_attempts\": 3,\n",
    "    \"retry_wait_min\": 10,\n",
    "    \"retry_wait_max\": 60,\n",
    "    \"sample_docs\": [\n",
    "        \"Paris est la capitale de la France, avec une population d'environ 2,2 millions d'habitants.\",\n",
    "        \"Lyon est la troisième plus grande ville de France, connue pour sa gastronomie et son histoire.\",\n",
    "        \"Le train TGV relie Paris à Lyon en environ 2 heures.\",\n",
    "        \"La Tour Eiffel est un monument emblématique de Paris, construit en 1889.\",\n",
    "        \"Mistral AI est une entreprise française spécialisée dans l'IA, fondée en 2023.\"\n",
    "    ]\n",
    "}"
   ],
   "id": "42e8f7433aeb5b",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Wrapper API Mistral",
   "id": "16d24985be334fd8"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T11:58:20.363904Z",
     "start_time": "2025-11-07T11:58:20.361113Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class MistralClient:\n",
    "    def __init__(self):\n",
    "        self.api_key = MISTRAL_API_KEY\n",
    "        self.model = CONFIG[\"model\"]\n",
    "        self.base_url = CONFIG[\"base_url\"]\n",
    "\n",
    "    @retry(stop=stop_after_attempt(CONFIG[\"retry_attempts\"]),\n",
    "           wait=wait_exponential(multiplier=1, min=CONFIG[\"retry_wait_min\"], max=CONFIG[\"retry_wait_max\"]))\n",
    "    def _request(self, endpoint, payload):\n",
    "        headers = {\"Authorization\": f\"Bearer {self.api_key}\", \"Content-Type\": \"application/json\"}\n",
    "        url = f\"{self.base_url}{endpoint}\"\n",
    "        response = httpx.post(url, json=payload, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        return response.json()\n",
    "\n",
    "    def chat_completion(self, messages, temperature=None, max_tokens=None, **kwargs):\n",
    "        if temperature is None:\n",
    "            temperature = CONFIG[\"default_temperature\"]\n",
    "        if max_tokens is None:\n",
    "            max_tokens = CONFIG[\"default_max_tokens\"]\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"messages\": messages,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            **kwargs\n",
    "        }\n",
    "        response = self._request(CONFIG[\"chat_endpoint\"], payload)\n",
    "        return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "    def completion(self, prompt, temperature=None, max_tokens=None, **kwargs):\n",
    "        if temperature is None:\n",
    "            temperature = CONFIG[\"default_temperature\"]\n",
    "        if max_tokens is None:\n",
    "            max_tokens = CONFIG[\"default_max_tokens\"]\n",
    "        payload = {\n",
    "            \"model\": self.model,\n",
    "            \"prompt\": prompt,\n",
    "            \"temperature\": temperature,\n",
    "            \"max_tokens\": max_tokens,\n",
    "            **kwargs\n",
    "        }\n",
    "        response = self._request(\"/completions\", payload)\n",
    "        return response[\"choices\"][0][\"text\"]\n",
    "\n",
    "    def embeddings(self, input_texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"\n",
    "        Génère des embeddings pour une liste de textes en utilisant l'API Mistral.\n",
    "\n",
    "        Args:\n",
    "            input_texts (list): Liste de textes à embedder.\n",
    "\n",
    "        Returns:\n",
    "            list: Liste des embeddings.\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            \"model\": CONFIG[\"embed_model\"],\n",
    "            \"input\": input_texts\n",
    "        }\n",
    "        response = self._request(\"/embeddings\", payload)\n",
    "        return [emb[\"embedding\"] for emb in response[\"data\"]]\n"
   ],
   "id": "4af053346fadc719",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T11:58:20.367828Z",
     "start_time": "2025-11-07T11:58:20.366364Z"
    }
   },
   "cell_type": "code",
   "source": [
    "SCRATCHPAD_FORMAT = \"\"\"\n",
    "Utilisez le scratchpad suivant pour raisonner étape par étape :\n",
    "\n",
    "Scratchpad :\n",
    "- Étape 1 : Analyser le problème et identifier les éléments clés.\n",
    "- Étape 2 : Décomposer le problème en sous-tâches logiques.\n",
    "- Étape 3 : Résoudre chaque sous-tâche en utilisant des faits ou des raisonnements.\n",
    "- Étape 4 : Synthétiser les résultats et vérifier la cohérence.\n",
    "\n",
    "Réponse finale :\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def format_prompt_with_scratchpad(user_query):\n",
    "    \"\"\"\n",
    "    Formate un prompt utilisateur avec le scratchpad structuré.\n",
    "    \"\"\"\n",
    "    return f\"{SCRATCHPAD_FORMAT}\\n\\nQuestion : {user_query}\"\n"
   ],
   "id": "84bbd4e6771cf4d",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Implémentation du Planner\n",
    "\n",
    "Le planner prend la requête utilisateur et la décompose en une liste d'étapes logiques pour résoudre le problème. Cela suit le pattern décrit dans le guide pour une décomposition automatique.\n"
   ],
   "id": "521e93d68d405b1d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T11:58:20.371489Z",
     "start_time": "2025-11-07T11:58:20.369697Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def planner(query):\n",
    "    \"\"\"\n",
    "    Décompose la tâche en étapes numérotées courtes en utilisant l'API Mistral.\n",
    "\n",
    "    Args:\n",
    "        query (str): La requête utilisateur.\n",
    "\n",
    "    Returns:\n",
    "        list: Liste des étapes du plan.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Tu es un planner. Décompose la tâche en étapes numérotées courtes.\n",
    "\n",
    "Tâche: {query}\n",
    "\n",
    "Réponds uniquement avec du JSON valide, sans texte supplémentaire : {{\"plan\": [\"étape 1\", \"étape 2\", ...]}}\n",
    "\n",
    "Assure-toi que les étapes sont logiques et séquentielles.\n",
    "\"\"\"\n",
    "    client = MistralClient()\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    raw = client.chat_completion(messages, temperature=0.0)\n",
    "    try:\n",
    "        # Essayer de parser directement\n",
    "        return json.loads(raw)[\"plan\"]\n",
    "    except json.JSONDecodeError:\n",
    "        # Essayer d'extraire JSON du texte\n",
    "        json_match = re.search(r'\\{.*\\}', raw, re.DOTALL)\n",
    "        if json_match:\n",
    "            try:\n",
    "                return json.loads(json_match.group())[\"plan\"]\n",
    "            except:\n",
    "                pass\n",
    "        # Fallback : plan simple\n",
    "        return [\"Analyser la tâche\", \"Décomposer en étapes\", \"Résoudre chaque étape\", \"Synthétiser le résultat\"]\n"
   ],
   "id": "8b72dfe5938d4036",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Outils disponibles\n",
    "\n",
    "Définition des outils que le modèle peut appeler via des actions. Cela inclut un calculateur simple et une recherche mock. Pour un environnement de production, ajoutez du sandboxing pour l'exécution de code.\n"
   ],
   "id": "55f179d4508e098b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T11:58:20.376258Z",
     "start_time": "2025-11-07T11:58:20.374121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def execute_tool(action):\n",
    "    \"\"\"\n",
    "    Exécute une action d'outil basée sur la commande reçue.\n",
    "\n",
    "    Args:\n",
    "        action (str): La commande d'action, e.g., \"CALC: 2+2\" ou \"SEARCH: query\".\n",
    "\n",
    "    Returns:\n",
    "        str: L'observation résultante.\n",
    "    \"\"\"\n",
    "    if action.startswith(\"CALC:\"):\n",
    "        expr = action[5:].strip()\n",
    "        try:\n",
    "            # Utilisation d'eval avec un environnement sécurisé (seulement opérations mathématiques de base)\n",
    "            safe_dict = {\n",
    "                \"__builtins__\": {},\n",
    "                \"abs\": abs,\n",
    "                \"min\": min,\n",
    "                \"max\": max,\n",
    "                \"sum\": sum,\n",
    "                \"pow\": pow,\n",
    "                \"round\": round\n",
    "            }\n",
    "            result = eval(expr, safe_dict, {})\n",
    "            return f\"OBSERVATION: {result}\"\n",
    "        except (ValueError, SyntaxError, NameError, TypeError) as e:\n",
    "            return f\"OBSERVATION: Erreur dans le calcul - {str(e)}\"\n",
    "    elif action.startswith(\"SEARCH:\"):\n",
    "        query = action[7:].strip()\n",
    "        # Implémentation RAG réelle\n",
    "        results = rag_search(query)\n",
    "        return f\"OBSERVATION: {results}\"\n",
    "    else:\n",
    "        return \"OBSERVATION: Action inconnue\"\n"
   ],
   "id": "48c63ce8d077da1",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Implémentation RAG avec ChromaDB\n",
    "\n",
    "Configuration de la base de données vectorielle pour la recherche augmentée par récupération (RAG). Utilise ChromaDB avec des embeddings Mistral.\n"
   ],
   "id": "8d695045e54b83e2"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T11:58:25.339601Z",
     "start_time": "2025-11-07T11:58:20.378197Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class MistralEmbeddingFunction:\n",
    "    def __init__(self):\n",
    "        self.client = MistralClient()\n",
    "\n",
    "    def __call__(self, input: List[str]) -> List[List[float]]:\n",
    "        return self.client.embeddings(input)\n",
    "\n",
    "    def embed_documents(self, texts: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Génère des embeddings pour des documents.\"\"\"\n",
    "        return self.client.embeddings(texts)\n",
    "\n",
    "    def embed_query(self, input: List[str]) -> List[List[float]]:\n",
    "        \"\"\"Génère un embedding pour une requête unique.\"\"\"\n",
    "        return self.client.embeddings(input)\n",
    "\n",
    "    def name(self):\n",
    "        return \"mistral_embed\"\n",
    "\n",
    "\n",
    "# Initialisation de ChromaDB\n",
    "chroma_client = chromadb.PersistentClient(path=CONFIG[\"chroma_path\"])\n",
    "collection = chroma_client.get_or_create_collection(\n",
    "    name=\"knowledge_base\",\n",
    "    embedding_function=MistralEmbeddingFunction()\n",
    ")\n",
    "\n",
    "# Ajout de documents d'exemple (remplacez par vos propres documents)\n",
    "sample_docs = CONFIG[\"sample_docs\"]\n",
    "\n",
    "collection.add(\n",
    "    documents=sample_docs,\n",
    "    ids=[f\"doc_{i}\" for i in range(len(sample_docs))]\n",
    ")\n",
    "\n",
    "\n",
    "def rag_search(query, n_results=None):\n",
    "    \"\"\"\n",
    "    Effectue une recherche RAG en utilisant ChromaDB.\n",
    "\n",
    "    Args:\n",
    "        query (str): La requête de recherche.\n",
    "        n_results (int): Nombre de résultats à retourner.\n",
    "\n",
    "    Returns:\n",
    "        str: Résultats concaténés.\n",
    "    \"\"\"\n",
    "    if n_results is None:\n",
    "        n_results = CONFIG[\"rag_n_results\"]\n",
    "    results = collection.query(query_texts=[query], n_results=n_results)\n",
    "    docs = results[\"documents\"][0]\n",
    "    return \" \".join(docs)"
   ],
   "id": "3e523cd9fc42bd20",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exécution des Étapes\n",
    "\n",
    "Cette section définit la fonction pour exécuter chaque étape du plan, en utilisant le pattern ReAct pour raisonner, appeler des outils et collecter des observations."
   ],
   "id": "4a708c1e7976316a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T11:58:25.399360Z",
     "start_time": "2025-11-07T11:58:25.397122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def run_step(step, context):\n",
    "    \"\"\"\n",
    "    Exécute une étape donnée avec le contexte fourni.\n",
    "\n",
    "    Args:\n",
    "        step (str): La description de l'étape.\n",
    "        context (dict): Le contexte incluant le scratchpad précédent.\n",
    "\n",
    "    Returns:\n",
    "        dict: Le résultat de l'étape avec pensée, action, observation et résultat.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"Tu es un assistant de raisonnement utilisant le pattern ReAct.\n",
    "\n",
    "Pour cette étape: {step}\n",
    "\n",
    "Contexte précédent: {json.dumps(context, ensure_ascii=False)}\n",
    "\n",
    "Pense à ce que tu dois faire. Si tu as besoin d'un outil, spécifie l'action à prendre (par exemple: CALC: 2+2 ou SEARCH: population de Paris). Sinon, réponds directement.\n",
    "\n",
    "Format de réponse attendu (uniquement la pensée et l'action) :\n",
    "Pensée: [Ta pensée]\n",
    "Action: [Ton action ou ta réponse finale]\n",
    "\"\"\"\n",
    "    client = MistralClient()\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    resp = client.chat_completion(messages)\n",
    "\n",
    "    thought_match = re.search(r\"Pensée: (.*)\", resp)\n",
    "    action_match = re.search(r\"Action: (.*)\", resp)\n",
    "\n",
    "    thought = thought_match.group(1).strip() if thought_match else \"\"\n",
    "    action = action_match.group(1).strip() if action_match else resp\n",
    "\n",
    "    if action.startswith(\"CALC:\") or action.startswith(\"SEARCH:\"):\n",
    "        observation = execute_tool(action)\n",
    "        result = observation\n",
    "    else:\n",
    "        observation = \"\"\n",
    "        result = action\n",
    "\n",
    "    return {\n",
    "        \"thought\": thought,\n",
    "        \"action\": action,\n",
    "        \"observation\": observation,\n",
    "        \"result\": result\n",
    "    }\n"
   ],
   "id": "d38c74c709930341",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Verifier\n",
    "\n",
    "Le verifier vérifie la cohérence et la validité du résultat d'une étape. Pour l'instant, une vérification simple : présence d'un résultat.\n"
   ],
   "id": "fc2239a64169fe15"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T11:58:25.403062Z",
     "start_time": "2025-11-07T11:58:25.401620Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def verify(step_out):\n",
    "    \"\"\"\n",
    "    Vérifie si le résultat de l'étape est valide.\n",
    "\n",
    "    Args:\n",
    "        step_out (dict): Le dictionnaire de sortie de l'étape.\n",
    "\n",
    "    Returns:\n",
    "        bool: True si valide, False sinon.\n",
    "    \"\"\"\n",
    "    return \"result\" in step_out and step_out[\"result\"].strip() != \"\"\n"
   ],
   "id": "10b7b5232ad98f64",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Modèles Pydantic\n",
    "\n",
    "Définition des schémas pour valider les sorties des étapes et du raisonnement.\n"
   ],
   "id": "3f1a85a46d86d410"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T11:58:25.407242Z",
     "start_time": "2025-11-07T11:58:25.405188Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "class StepOutput(BaseModel):\n",
    "    thought: str\n",
    "    action: str\n",
    "    observation: Optional[str] = None\n",
    "    result: Optional[str] = None\n",
    "\n",
    "\n",
    "class ReasoningOutput(BaseModel):\n",
    "    plan: List[str]\n",
    "    scratchpad: List[StepOutput]\n",
    "    final_answer: str\n"
   ],
   "id": "d735572f437cc337",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Logging et Métriques\n",
    "\n",
    "Utilisation de pandas pour stocker les logs des raisonnements et rich pour l'affichage.\n"
   ],
   "id": "77944a5d0a80ce6"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T11:58:25.413294Z",
     "start_time": "2025-11-07T11:58:25.409160Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# DataFrame pour les métriques\n",
    "metrics_df = pd.DataFrame(columns=[\"query\", \"plan_length\", \"steps_count\", \"final_answer_length\", \"time_taken\"])\n",
    "\n",
    "console = Console()\n",
    "\n",
    "\n",
    "def log_reasoning(query, result, time_taken):\n",
    "    \"\"\"\n",
    "    Log les métriques d'un raisonnement.\n",
    "\n",
    "    Args:\n",
    "        query (str): La requête.\n",
    "        result (dict): Le résultat du raisonnement.\n",
    "        time_taken (float): Temps pris en secondes.\n",
    "    \"\"\"\n",
    "    global metrics_df\n",
    "    new_row = {\n",
    "        \"query\": query,\n",
    "        \"plan_length\": len(result[\"plan\"]),\n",
    "        \"steps_count\": len(result[\"scratchpad\"]),\n",
    "        \"final_answer_length\": len(result[\"final_answer\"]),\n",
    "        \"time_taken\": time_taken\n",
    "    }\n",
    "    metrics_df.loc[len(metrics_df)] = new_row\n",
    "\n",
    "\n",
    "def display_metrics():\n",
    "    \"\"\"\n",
    "    Affiche les métriques avec rich.\n",
    "    \"\"\"\n",
    "    table = Table(title=\"Métriques de Raisonnement\")\n",
    "    table.add_column(\"Requête\", style=\"cyan\", no_wrap=True)\n",
    "    table.add_column(\"Longueur Plan\", justify=\"right\")\n",
    "    table.add_column(\"Nombre Étapes\", justify=\"right\")\n",
    "    table.add_column(\"Longueur Réponse\", justify=\"right\")\n",
    "    table.add_column(\"Temps (s)\", justify=\"right\")\n",
    "\n",
    "    for _, row in metrics_df.iterrows():\n",
    "        table.add_row(\n",
    "            row[\"query\"][:50] + \"...\" if len(row[\"query\"]) > 50 else row[\"query\"],\n",
    "            str(row[\"plan_length\"]),\n",
    "            str(row[\"steps_count\"]),\n",
    "            str(row[\"final_answer_length\"]),\n",
    "            f\"{row['time_taken']:.2f}\"\n",
    "        )\n",
    "\n",
    "    console.print(table)\n"
   ],
   "id": "b8409f7388d25e2c",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Perform Reasoning\n",
    "\n",
    "Rassemble toutes les étapes pour effectuer le raisonnement complet sur une requête utilisateur.\n"
   ],
   "id": "ea276c8d708409ed"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T11:58:25.417231Z",
     "start_time": "2025-11-07T11:58:25.415295Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "def perform_reasoning(query, max_tokens=None):\n",
    "    \"\"\"\n",
    "    Effectue le raisonnement complet sur une requête utilisateur.\n",
    "\n",
    "    Args:\n",
    "        query (str): La requête utilisateur.\n",
    "        max_tokens (int): Nombre maximum de tokens pour les réponses (optionnel).\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionnaire contenant le plan, le scratchpad et la réponse finale.\n",
    "    \"\"\"\n",
    "    if max_tokens is None:\n",
    "        max_tokens = CONFIG[\"default_max_tokens\"]\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Étape 1 : Planning\n",
    "    plan = planner(query)\n",
    "    time.sleep(2)  # Pause pour éviter le rate limiting\n",
    "\n",
    "    # Initialisation du scratchpad\n",
    "    scratchpad = []\n",
    "\n",
    "    # Étape 2 : Exécution des étapes\n",
    "    for step in plan:\n",
    "        context = {\"scratchpad\": scratchpad}\n",
    "        step_out = run_step(step, context)\n",
    "        scratchpad.append(step_out)\n",
    "\n",
    "        # Vérification simple\n",
    "        if not verify(step_out):\n",
    "            # Réflexion simple : marquer comme erreur\n",
    "            step_out[\"result\"] = \"Erreur détectée, étape à corriger\"\n",
    "        time.sleep(2)  # Pause entre les étapes\n",
    "\n",
    "    # Étape 3 : Agrégation de la réponse finale\n",
    "    final_answer = \" \".join([s.get(\"result\", \"\") for s in scratchpad if s.get(\"result\")])\n",
    "\n",
    "    end_time = time.time()\n",
    "    time_taken = end_time - start_time\n",
    "\n",
    "    result = {\n",
    "        \"plan\": plan,\n",
    "        \"scratchpad\": scratchpad,\n",
    "        \"final_answer\": final_answer\n",
    "    }\n",
    "\n",
    "    # Log les métriques\n",
    "    log_reasoning(query, result, time_taken)\n",
    "\n",
    "    return result\n"
   ],
   "id": "470d9364092dc3d3",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Exemples d'utilisation\n",
    "\n",
    "Voici quelques exemples pour tester la couche de raisonnement. Assurez-vous que votre clé API Mistral est configurée dans le fichier .env.\n"
   ],
   "id": "7ba69fdf7f32ad9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:02:32.877624Z",
     "start_time": "2025-11-07T11:58:25.419121Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Exemple 1 : Calcul mathématique simple\n",
    "print(\"Exemple 1 : Calcul de 123 * 47\")\n",
    "result1 = perform_reasoning(\"Calcule 123 * 47 et explique les étapes.\")\n",
    "print(\"Plan :\", result1[\"plan\"])\n",
    "print(\"Réponse finale :\", result1[\"final_answer\"])\n",
    "print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "# Exemple 2 : Recherche avec RAG\n",
    "print(\"Exemple 2 : Recherche d'informations sur Paris\")\n",
    "result2 = perform_reasoning(\"Trouve des informations sur Paris.\")\n",
    "print(\"Plan :\", result2[\"plan\"])\n",
    "print(\"Réponse finale :\", result2[\"final_answer\"])\n",
    "print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "# Exemple 3 : Tâche plus complexe\n",
    "print(\"Exemple 3 : Planification d'un voyage\")\n",
    "result3 = perform_reasoning(\"Planifie un voyage Paris-Lyon en minimisant le temps.\")\n",
    "print(\"Plan :\", result3[\"plan\"])\n",
    "print(\"Réponse finale :\", result3[\"final_answer\"])\n",
    "print(\"\\n\" + \"=\" * 50 + \"\\n\")\n",
    "\n",
    "# Affichage des métriques\n",
    "display_metrics()\n"
   ],
   "id": "280f0cbf3cd861ac",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple 1 : Calcul de 123 * 47\n",
      "Plan : ['Décomposer 47 en 40 + 7', 'Multiplier 123 par 40', 'Multiplier 123 par 7', 'Additionner les résultats des étapes 2 et 3']\n",
      "Réponse finale : 47 = 40 + 7 OBSERVATION: 492 OBSERVATION: 861 OBSERVATION: 1353\n",
      "\n",
      "==================================================\n",
      "\n",
      "Exemple 2 : Recherche d'informations sur Paris\n",
      "Plan : ['1. Définir les informations spécifiques à rechercher (géographie, histoire, culture, etc.)', '2. Utiliser des sources fiables (sites officiels, encyclopédies, guides touristiques)', '3. Rechercher des informations générales sur Paris (localisation, population, etc.)', \"4. Explorer l'histoire de Paris (fondation, événements marquants)\", '5. Découvrir les monuments et lieux emblématiques', '6. Identifier les aspects culturels (musées, festivals, gastronomie)', '7. Vérifier les informations pour leur exactitude', '8. Organiser les données collectées par thème', '9. Compléter avec des détails pratiques (transports, hébergement, etc.)', '10. Synthétiser les informations en un résumé clair']\n",
      "Réponse finale : Quel est le sujet ou la thématique spécifique que vous souhaitez explorer (géographie, histoire, culture, etc.) ? Attendre la réponse de l'utilisateur pour le sujet ou la thématique à explorer. Attendre la réponse de l'utilisateur pour le sujet ou la thématique à explorer. OBSERVATION: La Tour Eiffel est un monument emblématique de Paris, construit en 1889. Paris est la capitale de la France, avec une population d'environ 2,2 millions d'habitants. Lyon est la troisième plus grande ville de France, connue pour sa gastronomie et son histoire. OBSERVATION: La Tour Eiffel est un monument emblématique de Paris, construit en 1889. Paris est la capitale de la France, avec une population d'environ 2,2 millions d'habitants. Lyon est la troisième plus grande ville de France, connue pour sa gastronomie et son histoire. Attendre la réponse de l'utilisateur pour le sujet ou la thématique culturelle à explorer. Attendre la réponse de l'utilisateur pour le sujet ou la thématique culturelle à explorer. Organiser les données collectées par thème comme suit : OBSERVATION: Paris est la capitale de la France, avec une population d'environ 2,2 millions d'habitants. Le train TGV relie Paris à Lyon en environ 2 heures. La Tour Eiffel est un monument emblématique de Paris, construit en 1889. Pensée: Le contexte précédent montre que l'utilisateur a exploré plusieurs aspects de Paris (histoire, monuments, transports, hébergement) mais sans synthèse claire. Je vais résumer les informations collectées de manière organisée par thème pour fournir une réponse finale structurée.\n",
      "\n",
      "Action:\n",
      "**Résumé sur Paris :**\n",
      "\n",
      "1. **Généralités :**\n",
      "   - Capitale de la France, population d'environ 2,2 millions d'habitants.\n",
      "   - Connue pour sa richesse historique, culturelle et architecturale.\n",
      "\n",
      "2. **Histoire :**\n",
      "   - Fondation antique (Lutèce), événements marquants non détaillés dans les observations.\n",
      "   - Monuments emblématiques comme la Tour Eiffel (1889).\n",
      "\n",
      "3. **Monuments et lieux emblématiques :**\n",
      "   - Tour Eiffel (symbole de Paris).\n",
      "   - Autres sites mentionnés indirectement (Louvre, Notre-Dame, etc.).\n",
      "\n",
      "4. **Transports et hébergement :**\n",
      "   - Réseau de transports développé (métro, bus, RER).\n",
      "   - Options d'hébergement variées (hôtels, Airbnb, auberges de jeunesse).\n",
      "\n",
      "5. **Comparaison avec Lyon :**\n",
      "   - Lyon est la 3ᵉ ville de France, réputée pour sa gastronomie et son histoire.\n",
      "\n",
      "**Prochaines étapes :** Si l'utilisateur souhaite approfondir un thème spécifique (ex : détails sur un monument, itinéraires de transport), je peux effectuer une recherche ciblée.\n",
      "\n",
      "==================================================\n",
      "\n",
      "Exemple 3 : Planification d'un voyage\n",
      "Plan : ['1. Vérifier les horaires des trains TGV et vols directs', '2. Comparer les durées de trajet (train vs avion)', '3. Réserver le moyen de transport le plus rapide', '4. Prévoir un trajet direct sans correspondance', '5. Prévoir un temps de trajet de 2h en TGV ou 1h15 en avion', \"6. Prévoir un temps de transport jusqu'à l'aéroport ou la gare\", \"7. Prévoir un temps d'enregistrement et de sécurité si vol\", \"8. Prévoir un temps de trajet depuis l'aéroport ou la gare jusqu'à la destination finale\"]\n",
      "Réponse finale : OBSERVATION: Le train TGV relie Paris à Lyon en environ 2 heures. Lyon est la troisième plus grande ville de France, connue pour sa gastronomie et son histoire. Paris est la capitale de la France, avec une population d'environ 2,2 millions d'habitants. OBSERVATION: Le train TGV relie Paris à Lyon en environ 2 heures. Lyon est la troisième plus grande ville de France, connue pour sa gastronomie et son histoire. Paris est la capitale de la France, avec une population d'environ 2,2 millions d'habitants. OBSERVATION: Le train TGV relie Paris à Lyon en environ 2 heures. Lyon est la troisième plus grande ville de France, connue pour sa gastronomie et son histoire. Paris est la capitale de la France, avec une population d'environ 2,2 millions d'habitants. OBSERVATION: Le train TGV relie Paris à Lyon en environ 2 heures. Lyon est la troisième plus grande ville de France, connue pour sa gastronomie et son histoire. Paris est la capitale de la France, avec une population d'environ 2,2 millions d'habitants. OBSERVATION: Le train TGV relie Paris à Lyon en environ 2 heures. Lyon est la troisième plus grande ville de France, connue pour sa gastronomie et son histoire. Paris est la capitale de la France, avec une population d'environ 2,2 millions d'habitants. OBSERVATION: Le train TGV relie Paris à Lyon en environ 2 heures. Lyon est la troisième plus grande ville de France, connue pour sa gastronomie et son histoire. Paris est la capitale de la France, avec une population d'environ 2,2 millions d'habitants. OBSERVATION: Le train TGV relie Paris à Lyon en environ 2 heures. Lyon est la troisième plus grande ville de France, connue pour sa gastronomie et son histoire. Paris est la capitale de la France, avec une population d'environ 2,2 millions d'habitants. OBSERVATION: Le train TGV relie Paris à Lyon en environ 2 heures. Lyon est la troisième plus grande ville de France, connue pour sa gastronomie et son histoire. Paris est la capitale de la France, avec une population d'environ 2,2 millions d'habitants.\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001B[3m                                             Métriques de Raisonnement                                             \u001B[0m\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n",
       "┃\u001B[1m                                                       \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m    Longueur\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m               \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m    Longueur\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m           \u001B[0m┃\n",
       "┃\u001B[1m \u001B[0m\u001B[1mRequête                                              \u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m        Plan\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mNombre Étapes\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1m     Réponse\u001B[0m\u001B[1m \u001B[0m┃\u001B[1m \u001B[0m\u001B[1mTemps (s)\u001B[0m\u001B[1m \u001B[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n",
       "│\u001B[36m \u001B[0m\u001B[36mCalcule 123 * 47 et explique les étapes.             \u001B[0m\u001B[36m \u001B[0m│            4 │             4 │           63 │     36.41 │\n",
       "│\u001B[36m \u001B[0m\u001B[36mTrouve des informations sur Paris.                   \u001B[0m\u001B[36m \u001B[0m│           10 │            10 │         2553 │     86.57 │\n",
       "│\u001B[36m \u001B[0m\u001B[36mPlanifie un voyage Paris-Lyon en minimisant le tem...\u001B[0m\u001B[36m \u001B[0m│            8 │             8 │         2023 │    124.45 │\n",
       "└───────────────────────────────────────────────────────┴──────────────┴───────────────┴──────────────┴───────────┘\n"
      ],
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-style: italic\">                                             Métriques de Raisonnement                                             </span>\n",
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━┳━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">                                                       </span>┃<span style=\"font-weight: bold\">     Longueur </span>┃<span style=\"font-weight: bold\">               </span>┃<span style=\"font-weight: bold\">     Longueur </span>┃<span style=\"font-weight: bold\">           </span>┃\n",
       "┃<span style=\"font-weight: bold\"> Requête                                               </span>┃<span style=\"font-weight: bold\">         Plan </span>┃<span style=\"font-weight: bold\"> Nombre Étapes </span>┃<span style=\"font-weight: bold\">      Réponse </span>┃<span style=\"font-weight: bold\"> Temps (s) </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━╇━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Calcule 123 * 47 et explique les étapes.              </span>│            4 │             4 │           63 │     36.41 │\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Trouve des informations sur Paris.                    </span>│           10 │            10 │         2553 │     86.57 │\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\"> Planifie un voyage Paris-Lyon en minimisant le tem... </span>│            8 │             8 │         2023 │    124.45 │\n",
       "└───────────────────────────────────────────────────────┴──────────────┴───────────────┴──────────────┴───────────┘\n",
       "</pre>\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Tests avec pytest\n",
    "\n",
    "Exécution de tests unitaires pour valider les composants du raisonnement.\n"
   ],
   "id": "4691eb676cc2f18e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-07T12:02:53.832052Z",
     "start_time": "2025-11-07T12:02:33.012778Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Tests simples (exécutez avec pytest dans un terminal séparé)\n",
    "def test_planner():\n",
    "    plan = planner(\"Calcule 2+2\")\n",
    "    assert isinstance(plan, list)\n",
    "    assert len(plan) > 0\n",
    "\n",
    "\n",
    "def test_execute_tool_calc():\n",
    "    obs = execute_tool(\"CALC: 2+2\")\n",
    "    assert obs == \"OBSERVATION: 4\"\n",
    "\n",
    "\n",
    "def test_execute_tool_search():\n",
    "    obs = execute_tool(\"SEARCH: Paris\")\n",
    "    assert \"OBSERVATION:\" in obs\n",
    "\n",
    "\n",
    "def test_verify():\n",
    "    step_out = {\"result\": \"some result\"}\n",
    "    assert verify(step_out) == True\n",
    "    step_out_empty = {\"result\": \"\"}\n",
    "    assert verify(step_out_empty) == False\n",
    "\n",
    "\n",
    "# Exécuter les tests\n",
    "if __name__ == \"__main__\":\n",
    "    test_planner()\n",
    "    test_execute_tool_calc()\n",
    "    test_execute_tool_search()\n",
    "    test_verify()\n",
    "    print(\"Tous les tests passent !\")\n"
   ],
   "id": "e3cee84c97829ae7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tous les tests passent !\n"
     ]
    }
   ],
   "execution_count": 14
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
